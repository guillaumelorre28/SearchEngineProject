
There's a consensus in this panel that the ideal web aggregation system 
combines technology and human trust.
 Gaping Void blogger Hugh MacLeod at SXSWi 2009 
"Everything starts with trust, whether I want to be found or want to find 
information," saidMicah Baldwin of Lijit Networks <http://www.lijit.com/>, who 
said the networks and tools we build to aggregate links replicate our trust 
relationships - so how much we trust the person who had recommended a link to 
us, or how reliable we regard a certain blogger or news site.

Melanie Baker of AideRSS <http://www.postrank.com/> said that if link blogs or 
aggregators are too 'human', they can be less trustworthy for some people. "Why 
should someone who doesn't know you take your opinion over theirs? We tried to 
tackle that using engagement metrics," she explained, so links recommended by 
people are backed up by how popular they have been with others.

How much is too much?

Several of the panel said the 'shared links' function in Google's Reader is 
really important because it adds a layer of social recommendation, that human 
'trust layer', to the automated results that the reader spits up.FriendFeed 
does the same, and though it generates a huge amount of information it's useful 
to regard it, said Baldwin, as "a gigantic river of information" that you can 
watch drift by each day, but it will show you trending topics and big stories.

There's a split in behaviour among news junkies; some prefer to tailor their 
sources of information exactly and check them all, while others like to 
subscribe to as much as possible, add new sources regularly and are satisfied 
to just to get through as much as they can. (I'm definitely in the first camp.)

It's important to remember when designing these services that your priorities 
may well be different to your readers. You're likely to be an 
information-overloaded news junkie, but your readers are probably people with 
time to spare who want to browse and read - those are the audiences of sites 
likeDigg and Reddit. Marshall Kirkpatrick of ReadWriteWeb says he has 400-500 
RSS feeds so can't get through them all, but likes to have them on the radar. 
Mere mortals shouldn't be afraid to click 'read all' if it all gets too much.

Opportunities for new aggregators

Gabe Rivera of TechMeme said there are still many subject areas without good 
link blogs, partly because there is not enough metadata around the stories in 
those sectors and admitted that he's looking at expanding beyond tech and 
politics (that'sTechmeme and Memeorandum <http://www.memeorandum.com/>) into 
the traditional business space.

"There are a lot of blogs covering business the economy, finance, stocks and 
personal finance but those are all different areas. Each sub-topic is probably 
too small for aggregation but some combination would be compelling," he said.

Baker disagreed and said there are aggregator sites and link blogs for very 
niche areas like atheist communities, but that they are less tech savvy and 
perhaps don't know how to get the same kind of exposure. It's a challenge for 
those sites to get mainstream enough that they reach what might be a non 
tech-savvy audience.

Multiple sources are good

Author Louis Gray said we shouldn't think of one definitive source for links 
though. If the foundation is trust and recommendation, and tech tools are used 
to organise those, then the phone could be just as relevant as a source of 
information. "Finding information is different for each of us because we have 
different goals, so the key to information overload comes down to what is 
relevant for us," said Gray.

Referral logs for your blog are another good source, links that contacts chose 
to share in Google's RSS Reader andTwitter which has proved such a good source 
of trusted, interesting links from friends that most of the audience admitted 
they have used their RSS reader less as a result.

Incidentally, despite the biggest ever audience at SXSWi this year with more 
than 10,000 delegates, there's a noticeable trend against blogging in favour of 
selecting key points and soundbites and posting to real-time networks like 
Twitter. That might be a good thing, if it makes it easier for readers to find 
the in-depth coverage and better for journalists who have to write longer 
reports, but Baldwin said if there are less in-depth, researched pieces that's 
not good in the long run.

Kirkpatrick also revealed that some of ReadWriteWeb's researchers have been 
carefully compiling a spreadsheet that explores the most bookmarked links from 
the site's stories, looks at the most 20 frequent and fastestDelicious users 
who bookmark them and then start regularly looking at those people's sites as 
sources of news. It's like checking incoming links to your blog, but then some. 
Brilliant.
 