
The problem with using the human voice to control computers is well known and 
well documented: it doesn’t always work. You can find yourself adopting the 
aggressive tone of a belligerent tourist in a foreign land while digital 
assistants employ a range of apologetic responses (“I’m sorry, I didn’t quite 
get that”, “I’m sorry, I didn’t understand the question”). We throw our arms up 
and complain about their shortcomings. Plenty of us have tried them, plenty of 
us have dismissed them as a waste of time.

Introducing the Guardian skill for Alexa
 Read more  
<https://www.theguardian.com/help/insideguardian/2016/sep/28/introducing-the-guardian-skill-for-alexa>
We tend not to hear about them doing the job perfectly well, because few 
people write impassioned tweets or blog posts about things that work 
flawlessly. The evidence, however, shows that we arebecoming more comfortable 
with using voice control 
<https://www.theguardian.com/technology/2016/jun/01/mary-meeker-voice-controlled-tech-boom-technology-predictions>
 as its capabilities improve. Back in May, Google announced that20% of mobile 
search queries were now initiated by voice 
<http://searchengineland.com/google-reveals-20-percent-queries-voice-queries-249917>
; and it is predicted that this will rise – across all platforms – to 50% by 
the end of the decade.

But it’s not phones leading the way in making voice control palatable. That 
honour goes to theAmazon Echo 
<https://www.theguardian.com/technology/2016/nov/13/amazon-echo-alexa-first-things-to-try>
, the home-based “ambient device” inhabited by a digital assistant called 
Alexa. Quietly launched in late 2014, the success of the Echo (and its smaller 
siblings, the Dot and the Tap) has been described as “unlikely”, but sales have 
been increasing steadily quarter by quarter, with an estimated five million 
units sold in the US alone. Compared to the average smartphone, the Echo is 
comparatively modest: it is mainly dedicated to playing music and only does 
anything if you call its name. “Alexa, play me a song by Hot Chip. Alexa, can I 
listen to Radio 2? Alexa, stop.” It does these things efficiently, without 
complaint, and in doing so engenders a strange kind of affection. “Alexa, 
goodnight.” “Goodnight,” it replies. “Sleep tight.”
 <> Facebook  
<https://www.facebook.com/dialog/share?app_id=180444840287&href=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3FCMP%3Dshare_btn_fb%26page%3Dwith%3Aimg-2%23img-2&picture=https%3A%2F%2Fmedia.guim.co.uk%2Fedcff7468636f7fe351267fcbf228ebf611f4461%2F0_53_3000_1800%2F3000.jpg>
Twitter  
<https://twitter.com/intent/tweet?text=%E2%80%8BHas%20voice%20control%20finally%20started%20speaking%20our%20language%E2%80%8B%3F&url=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3FCMP%3Dshare_btn_tw%26page%3Dwith%3Aimg-2%23img-2>
Pinterest  
<http://www.pinterest.com/pin/create/button/?description=%E2%80%8BHas%20voice%20control%20finally%20started%20speaking%20our%20language%E2%80%8B%3F&url=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3Fpage%3Dwith%3Aimg-2%23img-2&media=https%3A%2F%2Fmedia.guim.co.uk%2Fedcff7468636f7fe351267fcbf228ebf611f4461%2F0_53_3000_1800%2F3000.jpg>
 Cookery tips from Amazon Echo ... it only works if you call its name. 
Photograph: Rachel Murray/WireImage 
The first successes in getting computers to recognise the spoken word were 
made in the 1950s, but more significant ground was gained in the early 1970s 
when two students at Carnegie Mellon University,James and Janet Baker 
<http://newventurist.com/2013/07/innovation-in-speech-recognition/>, began to 
apply statistical modelling techniques to the recognition of speech patterns. 
These so-called “hidden Markov” models 
<http://www.nature.com/nbt/journal/v22/n10/full/nbt1004-1315.html> turned out 
to be perfectly suited to machine learning; by absorbing thousands of examples, 
the machine became capable (in theory) of handling examples that it hadn’t yet 
seen. The Bakers would go on to foundDragon Systems 
<https://books.google.co.uk/books?id=iqo28Z90-9cC&pg=PA110&lpg=PA110&dq=Carnegie+Mellon+University,+James+and+Janet+Baker&source=bl&ots=rEyxyDIkDn&sig=nCFxm8KYqDQrLh5YNRNYo45Ks0o&hl=en&sa=X&ved=0ahUKEwiZupTYw87QAhUiJ8AKHdBBA5k4ChDoAQgaMAA#v=onepage&q=Carnegie%20Mellon%20University%2C%20James%20and%20Janet%20Baker&f=false>
, which ultimately became Nuance, one of the architects of Apple’s Siri voice 
assistant. In the early days, Dragon’s software was used to power specialist 
accessibility and dictation applications, but when such techniques started 
being used in computer operating systems – most notably withApple’s PlainTalk 
<http://apple.wikia.com/wiki/PlainTalk> in 1993 – our difficult relationship 
with voice control began. The first version of PlainTalk hogged computer 
resources, understood a limited number of phrases and wasn’t particularly 
reliable. Ever since, we’ve subconsciously measured the effectiveness of each 
iteration of voice control in terms of the time we take to give up on it.

Increases in processor power made things better. “We were improving speech 
recognition year on year,” says Nils Lenke, senior director of corporate 
research at Nuance, “but it became tougher and tougher, because the old 
technology – the hidden Markov models – were nearing the end of their lifetime. 
When we started to use neural networks, accuracy went up a lot.”

The problem with machine learning techniques, according to Lenke, is that the 
model reflects what you show it. “You need a lot of data covering all kinds of 
variants of speech,” he says, “accents, dialects, ages, gender, and different 
settings, different environments. But when cloud-based speech recognition came 
along, things got a lot better; now, as people use it, we can see that data on 
our servers. The right data, covering exactly what people are doing with the 
technology. Not what we thought people might be doing.”
 <> Facebook  
<https://www.facebook.com/dialog/share?app_id=180444840287&href=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3FCMP%3Dshare_btn_fb%26page%3Dwith%3Aimg-3%23img-3&picture=https%3A%2F%2Fmedia.guim.co.uk%2F97c7f35cf1c31e8fae7fe0842583ab092a637408%2F75_0_2250_1350%2F2250.jpg>
Twitter  
<https://twitter.com/intent/tweet?text=%E2%80%8BHas%20voice%20control%20finally%20started%20speaking%20our%20language%E2%80%8B%3F&url=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3FCMP%3Dshare_btn_tw%26page%3Dwith%3Aimg-3%23img-3>
Pinterest  
<http://www.pinterest.com/pin/create/button/?description=%E2%80%8BHas%20voice%20control%20finally%20started%20speaking%20our%20language%E2%80%8B%3F&url=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3Fpage%3Dwith%3Aimg-3%23img-3&media=https%3A%2F%2Fmedia.guim.co.uk%2F97c7f35cf1c31e8fae7fe0842583ab092a637408%2F75_0_2250_1350%2F2250.jpg>
 Rich human-computer relationship ... Theodore consults ‘Samantha’ in Spike 
Jonze’s Her. Photograph: Allstar Collection/Warner Bros 
As speech recognition improves from 90% to 95% and beyond, the problem 
encountered by developers of voice assistants is not necessarily one of 
comprehension; it’s persuading us that it’s not just a novelty and that we 
should persist beyond uncovering its cute quirks. “Speech has to solve a 
problem,” says Lenke. “But which problems can it solve? Can I remember which 
ones it can solve and which it can’t? It can book a cinema ticket, but can it 
book a flight? We’re not yet able to build a speech recognition system that 
understands the world, and for human beings that’s difficult to understand.”

The car is perhaps the best example of a single “domain” with well-defined 
problems (finding petrol stations, demisting windscreens) that can now be dealt 
with by voice commands. But this kind of understated ambition has, perhaps 
unwittingly, been the Echo’s strength, too. “In terms of voice technology, it’s 
not revolutionary,” says Simon Bryant, associate director at Futuresource 
Consulting, “but people aren’t overwhelmed by it. They get it. The entrypoint 
is via controlling your media, but once you get comfortable with playing a 
track, or a radio station, and you’re aware that it’s constantly learning, 
other applications will piggyback on to that. The potential is huge.”
 <> Facebook  
<https://www.facebook.com/dialog/share?app_id=180444840287&href=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3FCMP%3Dshare_btn_fb%26page%3Dwith%3Aimg-4%23img-4&picture=https%3A%2F%2Fmedia.guim.co.uk%2Fd8909989788812c386b709e72577cbff1d530bcb%2F0_53_4000_2400%2F4000.jpg>
Twitter  
<https://twitter.com/intent/tweet?text=%E2%80%8BHas%20voice%20control%20finally%20started%20speaking%20our%20language%E2%80%8B%3F&url=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3FCMP%3Dshare_btn_tw%26page%3Dwith%3Aimg-4%23img-4>
Pinterest  
<http://www.pinterest.com/pin/create/button/?description=%E2%80%8BHas%20voice%20control%20finally%20started%20speaking%20our%20language%E2%80%8B%3F&url=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3Fpage%3Dwith%3Aimg-4%23img-4&media=https%3A%2F%2Fmedia.guim.co.uk%2Fd8909989788812c386b709e72577cbff1d530bcb%2F0_53_4000_2400%2F4000.jpg>
 A customer tries out an iPhone’s Siri at a Hong Kong Apple shop. Photograph: 
Jerome Favre/Bloomberg via Getty Images 
The Echo’s expandability comes in the form of “skills”, links with third-party 
services that range from time-wasting ephemera (trivia quizzes) to things that 
could be genuinely useful if you happened to have the right technology 
installed in your home (ie controlling room temperature). But the growing 
affection for the device is also linked to its immobility. Much of our 
interaction with a smartphone is conducted in public, where talking to it is 
simply too embarrassing. In the privacy of the home, however, we can explore 
its capabilities and slowly come to terms with the reality of talking to a 
machine.

With Apple’s Siri, personality is crucial; behind its development seems to be 
a belief that the psychological “rule of reciprocation” (where we mirror the 
behaviour others dish out) also applies to machines. Google Now, meanwhile, is 
cooler, more utilitarian. “Some people would say that building a persona makes 
people more comfortable and lowers the barrier to entry,” says Lenke. “Others 
would say look, it’s a machine, we should make that visible. Both approaches 
can be right, depending on the task.”

Ultimately, our enthusiasm for voice control may be defined by issues of 
trust. There’s trust in the device itself – that it will perform in the way we 
want it to – and there’s trust in the company providing the service. Privacy 
concerns are never far from debates surrounding voice control; to function 
properly it requires data to be processed on external servers, but warnings of 
this within terms and conditions (“if your spoken words include personal or 
other sensitive information, that information will be among the data captured 
and transmitted to a third party”) can end up being interpreted as an Orwellian 
nightmare.
 <> Facebook  
<https://www.facebook.com/dialog/share?app_id=180444840287&href=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3FCMP%3Dshare_btn_fb%26page%3Dwith%3Aimg-5%23img-5&picture=https%3A%2F%2Fmedia.guim.co.uk%2F681a97c3568b176c1dfa76adb13e452716b9b81e%2F315_0_1733_2023%2F1733.jpg>
Twitter  
<https://twitter.com/intent/tweet?text=%E2%80%8BHas%20voice%20control%20finally%20started%20speaking%20our%20language%E2%80%8B%3F&url=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3FCMP%3Dshare_btn_tw%26page%3Dwith%3Aimg-5%23img-5>
Pinterest  
<http://www.pinterest.com/pin/create/button/?description=%E2%80%8BHas%20voice%20control%20finally%20started%20speaking%20our%20language%E2%80%8B%3F&url=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2016%2Fdec%2F04%2Fvoice-control-amazon-echo-digital%3Fpage%3Dwith%3Aimg-5%23img-5&media=https%3A%2F%2Fmedia.guim.co.uk%2F681a97c3568b176c1dfa76adb13e452716b9b81e%2F315_0_1733_2023%2F1733.jpg>
 The stuff of sci-fi ... Gary Lockwood’s astronaut attempts to communicate with 
HAL in 2001: A Space Odyssey. Photograph: MGM/Everett/Rex Features 
These issues can be particularly sensitive when it comes to Amazon, whose 
ultimate aim with the Echo could reasonably be interpreted as “frictionless 
purchasing” – ie getting us to buy things as quickly and easily as possible. 
Detractors warn of an inglorious future where we casually mention that we’re 
out of washing up liquid, and an hour later a drone drops some off at our door 
having already billed our credit card. “Amazon’s strategy is far reaching,” 
says Simon Bryant, “but a big aspect of it is Amazon Prime, which drives 
consumption of products and keeps people paying on an annual basis.” Behind the 
cute replies and brisk efficiency of voice assistants is the aim of drawing us 
into an ecosystem; for example, the Echo’s ability to cue up sounds in response 
to a murmur is beautifully executed, but if your stash of personal music 
happens to be with Google Play, you’ll have to move everything over to Amazon’s 
Music Unlimited if you want to listen to it. The promise of voice control will 
continue to be restricted by these walled gardens.

Advances in speech recognition could be seen as the fulfilling of a science 
fiction dream that extends from Star Trek through2001: A Space Odyssey 
<https://www.youtube.com/watch?v=dSIKBliboIo> to Knight Rider and beyond. Its 
history has been characterised by disappointment, but its key attributes are 
clear: it is hands-free and fast, devices don’t have to be unlocked and there 
are no menu structures to navigate. As more TVs and set-top boxes become speech 
savvy, the remote control will be consigned to history. As devices get smaller 
and lose their keyboards and screens, voice control will become crucial. And 
according to Bryant, the knock-on effects are already being seen. “We’re 
expecting 6.1m units of Echo-like devices to be sold by the end of this year,” 
he says, “which takes a huge chunk out of the audio market. And it’s going to 
boost radio audiences, because people are going into rooms and just want 
something to be playing.”

Alexa’s ability to instantly switch on Heart FM falls well short of the kind 
of rich human-computer relationship that’s depicted inthe Spike Jonze film Her 
<https://www.youtube.com/watch?v=WzV6mXIOVl4>, but while new apps like Hound 
are becoming more adept at having longer conversations and understanding 
context, there are limits to a computer’s ability to deal with conversational 
interaction, according to Mark Bishop, professor of cognitive computing at 
Goldsmiths University of London. “Action-focused commands like ‘tell me the 
weather in Seattle’ are much simpler things for a machine to parse and interact 
with than an open-ended narrative,” he says. “But there are fundamental 
problems in AI that, for me, mean that we’re some years away from having a 
machine that can have a meaningful, goal-directed conversation, if it’s ever 
possible at all.”

We may not see a convincingly empathetic machine in our lifetime, but in the 
meantime we can always ask Alexa to “say something nice”. “You have a great 
taste in technology,” it replies. “But seriously, you rock. I’m glad to know 
you.” Same here, Alexa. I think.

No, Siri! A timeline of voice control 


1952 Scientists at Bell Laboratories build Audrey 
<https://astaspeaks.wordpress.com/2014/10/13/audrey-the-first-speech-recognition-system/>
, the first documented speech recognition system, which can discern between the 
numbers zero to nine

1962 The Shoebox 
<https://www-03.ibm.com/ibm/history/exhibits/specialprod1/specialprod1_7.html>, 
built at IBM, adds the words “plus”, “minus”, “total”, “subtotal”, “false” and 
“off” to the 10 digits and can perform simple arithmetic

1976 Work at Carnegie Mellon University results in Harpy 
<http://repository.cmu.edu/cgi/viewcontent.cgi?article=3352&context=compsci>, a 
system that can recognise 1,011 words to 90% accuracy from both male and female 
speakers

1986 IBM demonstrates a system running on “three 6ft-tall array processors” 
that can recognise 5,000 words at 95% accuracy

1990 Dragon <https://en.wikipedia.org/wiki/Dragon_NaturallySpeaking#History> 
launches a dictation software package for PC that recognises 30,000 words, but 
“discretely” (ie with pauses between words) for $4,995

1996 IBM’s MedSpeak <http://fl.pair.com/voice/vms.htm> combines software with 
a noise-cancelling microphone to achieve continuous speech recognition; 25,000 
words with an average accuracy of over 95%

2002 The newest version of Microsoft Word “allows you to literally speak to 
your computer via a microphone”

2007 GOOG-411 <https://en.wikipedia.org/wiki/GOOG-411>, a telephone-based 
directory service, is launched by Google; the following year sees the 
technology deployed in a Voice Search app for iPhone

2011 Siri 
<https://www.theguardian.com/technology/2011/oct/05/iphone-4s-siri-icloud>, 
previously available as a standalone app, is integrated into the operating 
system for the new iPhone 4S, launching the era of the digital assistant
 