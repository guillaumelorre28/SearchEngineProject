
Richard Feynman said that the easiest person to fool is yourself. Fooling 
yourself is a particular danger for the well-educated, who see themselves as 
smart; and who in all likelihood tend to hang around with like-minded people of 
similar background and experience. Because you’re smart, your ideas are 
necessarily good.

The danger here is that a self-reinforcing herd mentality arises. Ideas become 
customs, and customs become Truth. Any deviation from the Truth is to be 
quashed, and outsiders are mocked and derided. 

Sometimes the idea around which such a herd coalesces can arise from a 
charismatic leader: somebody like Brian Cox for example – or Donald Trump. The 
motivations of such leaders are varied and mixed. Some motives are honourable – 
for example the desire to help people to live healthier lives; while others are 
more base – fame, power, money.

Either way, the idea and the motive become subsumed over time by herd 
mentality, the need for disciples of the leader to feel that they belong.

These themes are explored in depth in “Denying to the Grave 
<https://www.amazon.co.uk/Denying-Grave-Ignore-Facts-that/dp/0199396604/ref=nosim?tag=lablicom-21>
”, by the daughter and father team of Sara E Gorman and Jack M Gorman. They 
look at several different issues (e.g. vaccination, keeping handguns at home, 
GMOs, nuclear power) and examine the reasons why people – even, and perhaps 
especially, well-educated people – maintain beliefs that are not just wrong but 
may in fact be harmful. 

As I’ve argued previously, such harmful beliefs do not arise from ignorance 
<https://www.theguardian.com/science/occams-corner/2016/aug/23/scientists-losing-science-communication-skeptic-cox>
. The “knowledge gap” model of science communication is surely entirely 
discredited by now.

And yet, when we observe worrying developments in society – certain 
appointments by the president-elect of the United States, for example – there 
is an overwhelming tendency to try to batter sense into people by showing the 
data on climate change, on measles mortality rates, and so on.

This is important, but by itself simply does not work. Unless of course the 
intent is to signal to other members of your gang that you think the right way 
and you’re one of the good guys, in which case you’re winning in spades.

But for those people who for example are (in reality) putting their children 
at risk by not having them vaccinated, opposition in the form of ‘facts’ 
confirms their own belief that they are doing the right thing – guided and 
aided of course by the charlatans at the centre of the harmful, 
self-reinforcing meme of which they are victims. Opposition builds their sense 
of belonging, encouraging a sense of emotional, even financial, security.

Related: Why scientists are losing the fight to communicate science to the 
public | Occam's corner 
<https://www.theguardian.com/science/occams-corner/2016/aug/23/scientists-losing-science-communication-skeptic-cox>

Even more powerfully, there are strong evolutionary reasons why these beliefs 
persist even in the face of overwhelmingly contrary evidence. Apparent 
irrationality, argue the Gormans, can be a survival mechanism. Our species has 
got this far by making quick inferences based on scarce information, inferring 
causality where there may be none, and avoiding actions that have an 
infinitesimal risk of a nonetheless deadly outcome. We cannot easily escape our 
evolutionary chains.

So what can be done? 

Strategies for communicating with people who do not act on a particular 
consensus of scientific evidence have to take into account certain principles, 
including the fact that the well-educated make irrational health decisions; 
that it’s not about a lack of information; that people are uncomfortable with 
uncertainty and don’t understand risk; and that people respond more to emotion 
than to statistics.

There are a number of recommendations for science communicators in the book. 
Scientists must flood the internet with correct information – it’s a search 
engine battle. But they must also be more sensitive to difficulties in 
understanding causality and risk, in people’s discomfort with uncertainty and 
their own weaknesses. 

And yes, this is difficult for the well-educated, those who consider 
themselves keepers of the truth. But humility is essential: we all tend to 
think uncritically, and place emotion over reason. We need compassion, empathy 
and emotion in our communication. 

After all, we’re only human.
 