
The study of psychology is facing a crisis. A lot of research  doesn’t show 
the same results 
<https://www.theguardian.com/science/2015/aug/27/study-delivers-bleak-verdict-on-validity-of-psychology-experiment-results>
 when the experiment is repeated, and it is critical we address this problem. 
But theResearch Excellence Framework 
<https://www.theguardian.com/higher-education-network/2014/dec/17/ref-2014-why-is-it-such-a-big-deal>
 has led to a research culture which is suffocating attempts to stabilise 
psychology in particular, and science in general.

The Ref encourages universities to push for groundbreaking, novel, and 
exciting research in the form of 4* papers, but it does not reward the efforts 
of those who replicate studies. As universities gear up for the next Ref 
submission in 2021, many researchers will not even consider attempts to 
replicate results.

False information

The point of replicating a study is to test whether a statistically 
significant result will appear again if the experiment is repeated. Of course, 
a similar result may not appear – casting into question the validity of the 
results from the first experiment.

Last year, the Open Science Collaboration <https://osf.io/vmrgu/> attempted to 
replicate 100 studies from highly ranked psychological journals. While 97% of 
the original studies had a statistically significant result, just 36% of the 
replications had the same outcome.

Scientists aren't superheroes – failure is a valid result
 Read more  
<https://www.theguardian.com/higher-education-network/2016/jun/08/scientists-arent-superheroes-failure-is-a-valid-result>
Equally worrying: when an effect did appear, it was often much smaller than 
previously thought <http://science.sciencemag.org/content/349/6251/aac4716.full>
.

Recent data calls into question some widely influential findings in 
psychological science. These problems are not confined to psychology however – 
many findings published in scientific literature mayactually be false 
<http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124>.

Science is supposed to be self-correcting and reproducibility is a cornerstone 
of the scientific method. Yet, in the top 100 journals between 1900 and 2012, 
just 1.6% of studies <http://pps.sagepub.com/content/7/6/537> were 
replications. We simply aren’t invested in replicating findings. We all want to 
be good researchers and understand more about how the world works. So why are 
we so reluctant to check our conclusions are valid?

No incentive

One problem is the sheer volume of work – new publications emerge in the 
millions each year. But a larger issue is the systems we carry out our research 
in: US universities have a tenure model, where job security is achieved through 
high numbers of publications. In Australia, prestigious research fellowships 
are similarly focused on a “publish or perish” mentality.

Worryingly, many academics admit to engaging in at least one  
<http://pss.sagepub.com/content/23/5/524>questionable research practice in 
order to achieve publication. Examples of this include: coming up with a 
hypotheses after data is collected, stopping collecting data when an effect 
appears in case it disappears later, or only reporting the significant effects 
from collected data. Others simply fabricate data – Dutch psychologist Diederik 
Stapel shockingly falsified data from more than 50 studies 
<http://retractionwatch.com/2015/12/08/diederik-stapel-now-has-58-retractions/>.

 In the UK, the Ref ranks the published works of researchers according to 
their originality (how novel is the research?), significance (does it have 
practical or commercial importance?), and rigour (is the research technically 
sound?). Outputs are then awarded one to four stars.

While research publications with 1* rankings are only recognised nationally 
for their originality, significance, and rigour, 4* rankings are considered 
world-leading. The cumulative total of 3* and 4* papers determines research 
funding allocation and has a knock-on effect on institutional position in 
league tables and therefore attractiveness to students. Obviously, the more 
publications the better.

Novelty bias

On the face of it, the Ref works quite well because it rewards quality work – 
it is better to publish fewer articles containing highly original and 
significant work. Those who do criticise it argue that the assessment process 
is overlycomplex and expensive 
<http://blogs.lse.ac.uk/impactofsocialsciences/2011/06/10/ref-alternative-harzing-google-scholar/>
 and that universities can, and do,game the system 
<https://www.timeshighereducation.com/news/twenty-per-cent-contracts-rise-in-run-up-to-ref/2007670.article>
.

But we argue that these criticisms miss a bigger and more fundamental issue: 
the Ref completely undermines our efforts to produce a reliable body of 
knowledge.

Why? The focus on originality and accessibility – publications exploring new 
areas of research using new paradigms, and avoiding testing well-established 
theories – is the exact opposite of what science needs to be doing to solve the 
troubling replication crisis. According to Ref standards, replicating an 
already published piece of work is simply uninteresting.

Five reasons why the REF is not fit for purpose‬‬‬
 Read more  
<https://www.theguardian.com/higher-education-network/2014/dec/15/research-excellence-framework-five-reasons-not-fit-for-purpose>
A BMJ study has shown that in science abstracts published between 1974 and 
2015, the use of the words “groundbreaking”, “innovative”, and “novel”increased 
by 2500% <http://www.bmj.com/content/351/bmj.h6467>. This reflects a cultural 
shift in language, encouraged by systems like the Ref, rather than a change in 
science itself.

 With the next Ref just four years away, many researchers are effectively 
faced with a choice: be a good scientist, or be a successful academic who gets 
funding and a promotion.

Potential solutions

Researchers in psychology – the discipline truly at the heart of the 
replication crisis – are leading the way for change. We now have theSociety for 
the Improvement of Psychological Science <http://improvingpsych.org/>, 
initiatives encouraging a moreopen and collaborative  
<http://ttp//www.psychologicalscience.org/index.php/publications/journals/badges>
science and opportunities to improve training in research methods drawing upon 
theOpen Science Framework <https://osf.io/wfc6u/wiki/home/>. These programmes 
are gaining traction and becoming adopted across not only the UK, but the 
global research community.

Encouragingly, a growing number of psychologists are pushing for change, 
calling forpre-registration 
<https://www.theguardian.com/science/blog/2013/jun/05/trust-in-science-study-pre-registration>
 of work to encourage transparency, and consortium-basedundergraduate projects 
<https://thepsychologist.bps.org.uk/volume-29/march-2016/instilling-scientific-rigour-grassroots>
. If the Ref changed its criteria to reward such practices, it would do more 
for science than all of the 4* outputs combined.

Join the higher education network 
<http://preview.gutools.co.uk/higher-education-network#https://register.theguardian.com/higher-education/?CMP=dis_57>
 for more comment, analysis and job opportunities, direct to your inbox. Follow 
us on Twitter@gdnhighered <https://twitter.com/GdnHigherEd>. And if you have an 
idea for a story, please read ourguidelines 
<https://www.theguardian.com/higher-education-network/2015/oct/16/share-your-ideas-how-to-write-for-the-higher-education-network>
 and email your pitch to us athighereducationnetwork@theguardian.com 
<mailto:highereducationnetwork@theguardian.com>

 