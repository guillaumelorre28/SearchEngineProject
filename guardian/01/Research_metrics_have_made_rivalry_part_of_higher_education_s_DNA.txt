
Many academics now showcase their work on one or more “reputation” websites – 
Google Scholar <https://scholar.google.co.uk/>, ResearchGate 
<http://www.researchgate.net/>, Academia.edu <http://www.academia.edu/about> … 
They are as familiar with their h-index – a number that measures the quality of 
a researcher’s output – as they are with their phone numbers, and they are not 
ashamed to quote them in job or research applications.

They have learned to live with, if not love, these metrics, which rate the 
number of highly cited papers an academic has written. They seem solid compared 
with the old-fashioned peer review in which one lot of academics, anonymously 
of course, comments on the quality of the work of another lot.

Science, values and the limits of measurement
 Read more  
<https://www.theguardian.com/science/political-science/2015/jul/14/science-values-and-the-limits-of-measurement>
This obsession with quantification hardly comes as a surprise. Universities 
boast of their positions in various league tables, lumping together 
incommensurable metrics and weighting them whimsically. They are urged to focus 
on a small set of “key performance indicators” – both financial and academic – 
as measures of success or failure. If businesses (and now, sadly, universities) 
can be run according to the “numbers”, why not individual academic careers?

Two objections are usually raised. The first is that intellectual and 
scientific creativity cannot be, and therefore should not be, calibrated – 
still less should it treated as a zero-sum game in which there must always be 
winners (to be praised) and losers (to be shamed).

By its very nature, research is difficult to measure. It is impossible to 
“plan” good research: serendipity and time-scales make that impossible, 
particularly when resources are not distributed fairly. The new higher 
education minister,Jo Johnson 
<https://www.theguardian.com/higher-education-network/2015/may/12/jo-johnson-appointed-universities-and-science-minister>
, admitted as much in his first policy speech last month, when he questioned 
whether it was truly in the national interest to have such an intense focus on 
the Oxbridge-London golden triangle.

And opinions on what makes good research vary. I remember another 
(Conservative) minister asking me: “Surely you believe at least 25% of research 
in universities is rubbish?” I could only reply that, even if I did, we would 
never be able to agree on which 25%.

I remember a minister asking me: 'Surely you believe at least 25% of research 
in universities is rubbish?'

Sadly, despite this evident truth, it is now almost impossible not to regard 
research in terms of competition. Universities jostle for league-table 
advantage, individual scholars and scientists battle for h-index reputation. 
Rivalry is now part of higher education’s DNA.

The second objection is that metrics are too crude. Peer review can never be 
replaced. This is broadly the conclusion reached in a report for the Higher 
Education Funding Council for England that examined whether the nextResearch 
Excellence Framework (REF) 
<https://www.theguardian.com/science/political-science/2015/jul/27/in-defence-of-the-ref>
 – the quality measure for universities’ research work – could rely mainly on 
metrics.

This argument is familiar. Peer review is like democracy – a lousy system but 
better than all the others, although that does raise the interesting question 
of how democratic peer review really is.

Inevitably it is those with established – and orthodox – reputations who get 
asked to be the reviewers. Then there is the question of how “expert” peer 
reviewers really are – and how fair or even conscientious. Most academics have 
experienced brief and biased judgments on articles or research bids over which 
they have sweated blood.

Finally there is the question of scale. Peer review works much better for a 
specialised journal, with an equally specialised stable of reviewers, than it 
does for large multidisciplinary projects, where reviewers may find themselves 
having to rate research in subjects they don’t have as much expertise in.

The best defence of peer review is to be honest about these shortcomings, and 
to recognise that, in many fields outside the humanities, metrics have a lot to 
offer. Perhaps at the nextREF <https://www.theguardian.com/education/ref> more 
reliance should be placed on metrics, with panels operating more as moderators 
than as judges. It makes little sense to adopt an either-or approach.

In defence of the Research Excellence Framework
 Read more  
<https://www.theguardian.com/science/political-science/2015/jul/27/in-defence-of-the-ref>
But maybe there is a deeper problem: the more peer review is professionalised 
(or, at any rate, systematised) and the higher the stakes, the more unreliable 
it becomes as a method for selecting “winners”. The same goes for metrics. 
Swapping h-indices can be gentle rivalry in a collegial environment. But in the 
no-holds barred competition now foisted on higher education, metrics can be a 
menace because they attempt to measure things that cannot easily be measured.

The war between peer review and metrics is a phoney one. The real war is 
between those who believe that values of respect and trust, mutuality and 
solidarity, should guide the behaviour of universities, and academics, and 
those who are determined to impose an impoverished and impoverishing market 
ideology in higher education.
 